description: https://github.com/pytorch/pytorch/issues/37558
--- a/test/distributed/test_c10d.py
+++ b/test/distributed/test_c10d.py
@@ -2969,124 +2969,6 @@
         return F.softmax(x, dim=1)
 
 
-@requires_gloo()
-class ReducerTest(TestCase):
-    def setUp(self):
-        self.store = c10d.FileStore("/dev/null", 1)
-        self.process_group = c10d.ProcessGroupGloo(self.store, 0, 1)
-
-    def test_single_dtype_single_bucket(self):
-        model = ReducerModule()
-        parameters = list(model.parameters())
-        buckets = [list(range(len(parameters)))]
-        dist.Reducer([parameters], buckets, self.process_group)
-
-    def _create_mixed_precision_model(self):
-        model = ReducerModule()
-        model.float()
-        model.fc1.double()
-        return model
-
-    def test_multi_dtype_single_bucket(self):
-        model = self._create_mixed_precision_model()
-
-        # Raise if there are multiple types per bucket.
-        # In this case we create one bucket for all parameters.
-        with self.assertRaises(RuntimeError):
-            parameters = [list(model.parameters())]
-            buckets = [list(range(len(parameters[0])))]
-            dist.Reducer(parameters, buckets, self.process_group)
-
-    def test_multi_dtype_multi_bucket(self):
-        model = self._create_mixed_precision_model()
-        parameters = [list(model.parameters())]
-        group_by_type = groupby(
-            range(len(parameters[0])),
-            key=lambda i: parameters[0][i].type())
-        buckets = [list(indices) for _, indices in group_by_type]
-        dist.Reducer(parameters, buckets, self.process_group)
-
-    def _create_reducer_for_models(self, models):
-        parameters = [list(model.parameters()) for model in models]
-        group_by_type = groupby(
-            range(len(parameters[0])),
-            key=lambda i: parameters[0][i].type())
-        buckets = [list(indices) for _, indices in group_by_type]
-        return dist.Reducer(parameters, buckets, self.process_group)
-
-    def test_forward_backward_single_replica(self):
-        batch_size = 10
-        model = self._create_mixed_precision_model()
-        reducer = self._create_reducer_for_models([model])
-        loss = nn.CrossEntropyLoss()
-        input = torch.rand([batch_size, 2], dtype=torch.double)
-        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])
-        output = loss(model(input), target)
-        reducer.prepare_for_backward(output)
-        output.backward()
-
-    def test_forward_backward_multi_replica(self):
-        batch_size = 10
-        num_replicas = 2
-        models = [self._create_mixed_precision_model() for _ in range(num_replicas)]
-        reducer = self._create_reducer_for_models(models)
-        loss = nn.CrossEntropyLoss()
-        input = torch.rand([batch_size, 2], dtype=torch.double).chunk(num_replicas)
-        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])
-        outputs = [models[i](input[i]) for i in range(num_replicas)]
-        output = loss(torch.cat(outputs), target)
-        reducer.prepare_for_backward(output)
-        output.backward()
-
-        # The reducer will have reduced the gradients for all model replicas.
-        # Verify that they are equal across model replicas.
-        for parameters in zip(*[model.parameters() for model in models]):
-            for parameter in parameters:
-                self.assertEqual(parameters[0].grad, parameter.grad)
-
-    def test_forward_backward_unused_parameters(self):
-        batch_size = 10
-        model = self._create_mixed_precision_model()
-        reducer = self._create_reducer_for_models([model])
-        loss = nn.CrossEntropyLoss()
-        input = torch.rand([batch_size, 2], dtype=torch.double)
-        target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])
-        output = loss(model(input, use_fc3=False), target)
-
-        # Check that the grad of fc3 is not set.
-        self.assertEqual(None, model.fc3.weight.grad)
-
-        # Compute and accumulate gradients.
-        reducer.prepare_for_backward(output)
-        output.backward()
-
-        # The reducer will have marked the grad of fc3 as ready, because
-        # it doesn't show up in the autograd graph of `output`. Since fc3.weight
-        # is considered being globally unused, it will be kept untouched as None.
-        self.assertEqual(None, model.fc3.weight.grad)
-
-    def test_forward_backward_optimizer(self):
-        batch_size = 10
-        model = self._create_mixed_precision_model()
-        reducer = self._create_reducer_for_models([model])
-        loss = nn.CrossEntropyLoss()
-        optimizer = torch.optim.Adam(model.parameters())
-        for i in range(3):
-            input = torch.rand([batch_size, 2], dtype=torch.double)
-            target = torch.LongTensor([random.randrange(4) for _ in range(batch_size)])
-
-            # The `zero_grad` function calls `detach_` and `zero_` on the grad
-            # tensors of model parameters. If we tried to set the grad tensors
-            # to a view of the reducer's bucket tensors, this would blow up.
-            optimizer.zero_grad()
-
-            # Unused parameter only in the first iteration.
-            output = loss(model(input, use_fc3=(i > 0)), target)
-            reducer.prepare_for_backward(output)
-            output.backward()
-            optimizer.step()
-
-
 class ComputeBucketAssignmentTest(TestCase):
     def test_single_limit_single_dtype(self):
         tensors = [
