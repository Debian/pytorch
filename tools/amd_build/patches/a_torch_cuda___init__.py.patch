diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py
index 21227ba9c0..994c778c74 100644
--- a/torch/cuda/__init__.py
+++ b/torch/cuda/__init__.py
@@ -148,8 +148,6 @@ def _lazy_call(callable):
         # Don't store the actual traceback to avoid memory cycle
         _queued_calls.append((callable, traceback.format_stack()))
 
-_lazy_call(_check_capability)
-
 
 class DeferredCudaCallError(Exception):
     pass
@@ -195,9 +193,6 @@ def _lazy_init():
                 "Cannot re-initialize CUDA in forked subprocess. " + msg)
         _check_driver()
         torch._C._cuda_init()
-        _cudart = _load_cudart()
-        _cudart.cudaGetErrorName.restype = ctypes.c_char_p
-        _cudart.cudaGetErrorString.restype = ctypes.c_char_p
         # Some of the queued calls may reentrantly call _lazy_init();
         # we need to just return without initializing in that case.
         # However, we must not let any *other* threads in!
